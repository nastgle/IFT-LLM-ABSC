{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "add65585",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d30658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Mapping of source XML files to their destination folders and output filenames.\n",
    "# This dictionary drives the entire script.\n",
    "file_mappings = {\n",
    "    'data/processed/laptop_14': {\n",
    "        'data_training': 'data/raw/SemEval 2014 laptops - training.xml',\n",
    "        'data_validation': 'data/raw/SemEval 2014 laptops - validation.xml'\n",
    "    },\n",
    "    'data/processed/restaurants_14': {\n",
    "        'data_training': 'data/raw/SemEval 2014 restaurants - training.xml',\n",
    "        'data_validation': 'data/raw/SemEval 2014 restaurants - validation.xml'\n",
    "    },\n",
    "    'data/processed/restaurants_15': {\n",
    "        'data_training': 'data/raw/SemEval 2015 restaurants - training.xml',\n",
    "        'data_validation': 'data/raw/SemEval 2015 restaurants - validation.xml'\n",
    "    },\n",
    "    'data/processed/restaurants_16': {\n",
    "        'data_training': 'data/raw/SemEval16_Restaurants_Train.xml',\n",
    "        'data_validation': 'data/raw/SemEval16_Restaurants_Test.xml'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Starting the XML to CSV conversion process...\")\n",
    "\n",
    "# --- Main script execution starts here ---\n",
    "\n",
    "# Iterate through the top-level keys of the mapping, which represent the folders to be created.\n",
    "for output_folder, files in file_mappings.items():\n",
    "    # Create the destination folder (e.g., 'laptop_14') if it doesn't already exist.\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    print(f\"\\nCreated/Verified folder: '{output_folder}'\")\n",
    "\n",
    "    # Iterate through the files specified for the current folder.\n",
    "    for output_name, input_xml in files.items():\n",
    "        # Construct the full path for the output CSV file (e.g., 'laptop_14/data_training.csv').\n",
    "        output_csv_path = os.path.join(output_folder, f\"{output_name}.csv\")\n",
    "        \n",
    "        print(f\"  - Parsing '{input_xml}' -> '{output_csv_path}'\")\n",
    "\n",
    "        # Use a try-except block to handle potential errors like a missing file or malformed XML.\n",
    "        try:\n",
    "            tree = ET.parse(input_xml)\n",
    "            root = tree.getroot()\n",
    "        except (ET.ParseError, FileNotFoundError) as e:\n",
    "            print(f\"    ERROR: Could not process file {input_xml}. Reason: {e}\")\n",
    "            # If an error occurs, skip this file and continue with the next one.\n",
    "            continue\n",
    "\n",
    "        # Open the target CSV file in write mode.\n",
    "        with open(output_csv_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "            # Write the header row for the CSV.\n",
    "            csv_writer.writerow(['sentence', 'aspect_term', 'sentiment', 'from', 'to'])\n",
    "\n",
    "            # Determine how to find sentences based on the XML root tag.\n",
    "            # Some files use <sentences> as the root, others use <Reviews>.\n",
    "            sentences_to_process = []\n",
    "            if root.tag == 'sentences':\n",
    "                sentences_to_process = root.findall('sentence')\n",
    "            elif root.tag == 'Reviews':\n",
    "                for review in root.findall('Review'):\n",
    "                    sentences_tag = review.find('sentences')\n",
    "                    if sentences_tag is not None:\n",
    "                        sentences_to_process.extend(sentences_tag.findall('sentence'))\n",
    "\n",
    "            # Process each sentence that was found.\n",
    "            for sentence in sentences_to_process:\n",
    "                text_element = sentence.find('text')\n",
    "                if text_element is None or text_element.text is None:\n",
    "                    continue\n",
    "                sentence_text = text_element.text.strip()\n",
    "\n",
    "                # Handle the structure used in 2014 datasets: <aspectTerms><aspectTerm/></aspectTerms>\n",
    "                aspect_terms_element = sentence.find('aspectTerms')\n",
    "                if aspect_terms_element is not None:\n",
    "                    for aspect_term in aspect_terms_element.findall('aspectTerm'):\n",
    "                        term = aspect_term.get('term')\n",
    "                        polarity = aspect_term.get('polarity')\n",
    "                        from_ = aspect_term.get('from')\n",
    "                        to_ = aspect_term.get('to')\n",
    "                        if term and polarity:\n",
    "                            csv_writer.writerow([sentence_text, term, polarity, from_, to_])\n",
    "                    \n",
    "\n",
    "                # Handle the structure used in 2015/2016 datasets: <Opinions><Opinion/></Opinions>\n",
    "                opinions_element = sentence.find('Opinions')\n",
    "                if opinions_element is not None:\n",
    "                    for opinion in opinions_element.findall('Opinion'):\n",
    "                        target = opinion.get('target')\n",
    "                        polarity = opinion.get('polarity')\n",
    "                        from_ = aspect_term.get('from')\n",
    "                        to_ = aspect_term.get('to')\n",
    "                        # Write row only if the target is not 'NULL'\n",
    "                        if target and target.lower() != 'null' and polarity:\n",
    "                            csv_writer.writerow([sentence_text, target, polarity, from_, to_])\n",
    "\n",
    "print(\"\\nProcessing complete. All files have been converted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a7da99",
   "metadata": {},
   "source": [
    "### ZERO SHOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f135db26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "\n",
    "start = time.time()\n",
    "for model_ in ['meta-llama/Meta-Llama-3-8B-Instruct', 'meta-llama/Meta-Llama-3-70B-Instruct']:\n",
    "# Initialize the client with the model you want to use\n",
    "    client = InferenceClient(model=model_, token=\"HF_TOKEN\")\n",
    "\n",
    "    for dataset in ['laptop_14', 'restaurants_14', 'restaurants_15', 'restaurants_16']:\n",
    "        # interate through data_validation.csv\n",
    "        data_validation_path = os.path.join('data', 'processed', dataset, 'data_validation.csv')\n",
    "        with open(data_validation_path, 'r', newline='', encoding='utf-8') as infile:\n",
    "            csv_reader = csv.DictReader(infile)\n",
    "            \n",
    "            # Prepare the messages for the chat completion\n",
    "            sentiments_original = []\n",
    "            sentiments_predicted = []\n",
    "            # loop through ten first rows\n",
    "            for row in csv_reader:\n",
    "                sentence_text = row.get('sentence')\n",
    "                term = row.get('aspect_term')\n",
    "                original_polarity = row.get('sentiment')\n",
    "                from_ = row.get('from')\n",
    "                to_ = row.get('to')\n",
    "\n",
    "                if not sentence_text or not term or not original_polarity:\n",
    "                    continue\n",
    "\n",
    "                prompt = f\"\"\"\n",
    "        Instruction:\n",
    "        Analyze the sentiment of the aspect term within the given sentence. The aspect term is highlighted by quotes. Your answer must be one of the following three options: 'positive', 'negative', 'neutral'. Do not provide any explanation or other text.\n",
    "        Sentence:\n",
    "        \"{sentence_text}\"\n",
    "        Aspect Term:\n",
    "        \"{term}\"\n",
    "\n",
    "        Location of the aspect term in the sentence is from {from_} character to {to_} character.\n",
    "        \n",
    "        Sentiment:\n",
    "        \"\"\"\n",
    "                sentiments_original.append(original_polarity)\n",
    "\n",
    "                # Send the prompt\n",
    "                response = client.chat_completion(temperature=0, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "                sentiments_predicted.append(response.choices[0].message[\"content\"])\n",
    "                time.sleep(0.5)\n",
    "\n",
    "        # calculate accuracy\n",
    "        correct_predictions = sum(\n",
    "            1 for original, predicted in zip(sentiments_original, sentiments_predicted)\n",
    "            if original.lower() == predicted.lower()\n",
    "        )\n",
    "        accuracy = correct_predictions / len(sentiments_original) * 100\n",
    "\n",
    "        # calculate F1 score\n",
    "        from sklearn.metrics import f1_score\n",
    "        # Convert sentiments to numerical values for F1 score calculation\n",
    "        sentiment_map = {'positive': 1, 'negative': -1, 'neutral': 0}\n",
    "        y_true = [sentiment_map[sentiment.lower()] for sentiment in sentiments_original]\n",
    "        y_pred = [sentiment_map[sentiment.lower()] for sentiment in sentiments_predicted]\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "        print(f\"\\n\\n________________________\")\n",
    "        print(f\"Model: {model_}\")\n",
    "        print(f\"Dataset: {dataset}\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"\\n\\nTotal execution time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a479537b",
   "metadata": {},
   "source": [
    "### Data preparation for simSCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e666a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "# Load SimCSE model\n",
    "model = SentenceTransformer('princeton-nlp/sup-simcse-roberta-large')\n",
    "\n",
    "# the database should be converted to emebeddings:\n",
    "def convert_to_embeddings(input_csv, output_embeddings_path):\n",
    "    \"\"\"\n",
    "    Convert sentences in the input CSV to embeddings and save them to a file.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    with open(input_csv, 'r', newline='', encoding='utf-8') as infile:\n",
    "        csv_reader = csv.DictReader(infile)\n",
    "        for row in csv_reader:\n",
    "            sentence_text = row.get('sentence')\n",
    "            if sentence_text:\n",
    "                sentences.append(sentence_text)\n",
    "\n",
    "    # Compute embeddings\n",
    "    embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "    # Save embeddings to a file\n",
    "    torch.save(embeddings, output_embeddings_path)\n",
    "    print(f\"Embeddings saved to {output_embeddings_path}\")\n",
    "\n",
    "# Convert each dataset's validation CSV to embeddings\n",
    "for dataset in ['laptop_14', 'restaurants_14', 'restaurants_15', 'restaurants_16']:\n",
    "    input_csv_path = os.path.join('data', 'processed', dataset, 'data_validation.csv')\n",
    "    output_embeddings_path = os.path.join('data', 'processed', dataset, 'embeddings.pt')\n",
    "    convert_to_embeddings(input_csv_path, output_embeddings_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba8939d",
   "metadata": {},
   "source": [
    "### FEW SHOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe57d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Load the embeddings model for simSCE analysis\n",
    "model_simSce = SentenceTransformer('princeton-nlp/sup-simcse-roberta-large')\n",
    "\n",
    "for model_ in ['meta-llama/Meta-Llama-3-8B-Instruct', 'meta-llama/Meta-Llama-3-70B-Instruct', 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'meta-llama/Meta-Llama-3.3-70B-Instruct']:\n",
    "# Initialize the client with the model you want to use\n",
    "    client = InferenceClient(model=model_, token=\"HF_TOKEN\")\n",
    "\n",
    "    for dataset in ['laptop_14', 'restaurants_14', 'restaurants_15', 'restaurants_16']:\n",
    "        # interate through data_validation.csv\n",
    "        data_validation_path = os.path.join('data', 'processed', dataset, 'data_validation.csv')\n",
    "        with open(data_validation_path, 'r', newline='', encoding='utf-8') as infile:\n",
    "            csv_reader = csv.DictReader(infile)\n",
    "            \n",
    "            # Prepare the messages for the chat completion\n",
    "            sentiments_original = []\n",
    "            sentiments_predicted = []\n",
    "            # loop through ten first rows\n",
    "            for row in csv_reader:\n",
    "                sentence_text = row.get('sentence')\n",
    "                term = row.get('aspect_term')\n",
    "                original_polarity = row.get('sentiment')\n",
    "                from_ = row.get('from')\n",
    "                to_ = row.get('to')\n",
    "\n",
    "                if not sentence_text or not term or not original_polarity:\n",
    "                    continue\n",
    "                \n",
    "                # transform the current sentence text to embeddings\n",
    "                sentence_embeddings = model_simSce.encode(sentence_text, convert_to_tensor=True)\n",
    "\n",
    "                # find top 5 closest sentences to the current by comparing embeddings and calculating cosine similarities\n",
    "                embeding_database = os.path.join('data', 'processed', dataset, 'embeddings.pt')\n",
    "                embeddings = torch.load(embeding_database)\n",
    "                cosine_scores = util.pytorch_cos_sim(sentence_embeddings, embeddings)[0]\n",
    "\n",
    "                # Get the top 5 closest sentences\n",
    "                top_k = min(5, len(cosine_scores))\n",
    "                top_results = torch.topk(cosine_scores, k=top_k)\n",
    "\n",
    "                # Create a few-shot prompt with the top 5 closest sentences\n",
    "                few_shot_examples = []\n",
    "                for score, idx in zip(top_results[0], top_results[1]):\n",
    "                    # Get the sentence and its aspect term from the CSV\n",
    "                    with open(data_validation_path, 'r', newline='', encoding='utf-8') as infile:\n",
    "                        csv_reader = csv.DictReader(infile)\n",
    "                        for i, example_row in enumerate(csv_reader):\n",
    "                            if i == idx:\n",
    "                                example_sentence = example_row['sentence']\n",
    "                                example_term = example_row['aspect_term']\n",
    "                                example_polarity = example_row['sentiment']\n",
    "                                break\n",
    "                    few_shot_examples.append(f\"\"\"\n",
    "                    Sentence: \"{example_sentence}\"\n",
    "                    Aspect Term: \"{example_term}\"\n",
    "                    Location of the aspect term in the sentence is from {example_row['from']} character to {example_row['to']} character.\n",
    "                    Sentiment: {example_polarity}\n",
    "                    \"\"\")\n",
    "\n",
    "\n",
    "                # Prepare the final  prompt\n",
    "                few_shot_prompt = f\"\"\"\n",
    "                Instruction:\n",
    "                Analyze the sentiment of the aspect term within the given sentence. The aspect term is highlighted by quotes. Your answer must be one of the following three options: 'positive', 'negative', 'neutral'. Do not provide any explanation or other text.\n",
    "                ------------\n",
    "                Examples:\n",
    "                {''.join(few_shot_examples)}\n",
    "                ------------\n",
    "                Sentence: \"{sentence_text}\"\n",
    "                Aspect Term: \"{term}\"\n",
    "                Location of the aspect term in the sentence is from {from_} character to {to_} character.\n",
    "                Sentiment:\n",
    "                \"\"\"\n",
    "\n",
    "                # Send the prompt\n",
    "                response = client.chat_completion(temperature=0, messages=[{\"role\": \"user\", \"content\": few_shot_prompt}])\n",
    "\n",
    "                # Save the original and predicted polarities to the lists\n",
    "                sentiments_original.append(original_polarity)\n",
    "                sentiments_predicted.append(response.choices[0].message[\"content\"])\n",
    "                time.sleep(0.5)\n",
    "\n",
    "        # calculate accuracy\n",
    "        correct_predictions = sum(\n",
    "            1 for original, predicted in zip(sentiments_original, sentiments_predicted)\n",
    "            if original.lower() == predicted.lower()\n",
    "        )\n",
    "        accuracy = correct_predictions / len(sentiments_original) * 100\n",
    "\n",
    "        # calculate F1 score\n",
    "        from sklearn.metrics import f1_score\n",
    "        # Convert sentiments to numerical values for F1 score calculation\n",
    "        sentiment_map = {'positive': 1, 'negative': -1, 'neutral': 0}\n",
    "        y_true = [sentiment_map[sentiment.lower()] for sentiment in sentiments_original]\n",
    "        y_pred = [sentiment_map[sentiment.lower()] for sentiment in sentiments_predicted]\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "        print(f\"\\n\\n________________________\")\n",
    "        print(f\"Model: {model_}\")\n",
    "        print(f\"Dataset: {dataset}\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"\\n\\nTotal execution time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a8f069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
